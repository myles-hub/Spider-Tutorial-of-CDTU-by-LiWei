---
typora-root-url: images

---

# python 爬虫

## 1. 任务介绍

### 1.1 需求分析

- （1）网页链接

​        https://movie.douban.com/top25

- （2）爬取信息

​        爬取豆瓣电影 TOP250 的基本信息，包括电影的"名称"、"豆瓣评分"、"评价数"、"电影概况"、"电影链接"等。

- （3）TOP 250

<img src="/image-20200829213504840.png" alt="image-20200829213504840"  />

## 2. 爬虫初始

### 2.1 爬虫基础知识

- （1）什么是爬虫

​       网络爬虫，是一种按照一定规则，自动抓取互联网信息的程序或者脚本。由于互联网
数据的多样性和资源的有限性，根据用户需求定向抓取相关网页并分析已成为如今主流
的爬取策略。

- （2）爬虫可以做什么

​       你可以爬取妹子的图片，爬取自己想看的视频等等，只要你能通过浏览器访问的数据，都可以通过爬虫获取。

- （3）爬虫的本质是什么

​       模拟浏览器打开网页，获取网页中我们想要的那部分数据。

<img src="/image-20200829220616924.png" alt="image-20200829220616924" style="zoom:80%;" />

<img src="/image-20200829220936586.png" alt="image-20200829220936586" style="zoom:80%;" />

### 2.2 其他知识点

- 百度指数

​       使用百度指数，我们可以去查询下“某个关键词”近期被查询的热度清空。（尝试查询下”电影天堂“，”吃鸡“，”爱奇艺“去感受下。）

- 天眼查

  查询相关公司的基本信息清空。

- 天堂电影

​       电影天堂的数据细致分析下，就会发现其数据基本都是冲豆瓣或其他网站上爬取下来，然后重新展现的。



## 3. 基础流程

- （1）准备工作

​       通过浏览器查看分析目标网页，学习编程基础规范。

- （2）获取数据

​       通过HTTP库向目标站点发起请求，请求可以包含额外的header等信息，如
果服务器能正常响应，会得到一个Response，便是所要获取的页面内容。

- （3）解析内容

​       得到的内容可能是HTML、json等格式，可以用页面解析库、正则表达式等
进行解析。

- （4）保存数据

​       保存形式多样，可以存为文本，也可以保存到数据库，或者保存特定格式的文件。

### 3.1 准备工作

​         通过浏览器查看分析目标网页，学习编程基础规范。

<img src="/image-20200829221956201.png" alt="image-20200829221956201"  />

提示：本章节需要有HTML、CSS、JS 的基础知识。

#### 3.1.1 页面分析

​        学会使用“开发者调试工具”进行web网页的`Elements` 和 `network` 两个常用模块来进行数据位置定位和请求过程的分析。

- （1）开发者调试工具调用
  - 快捷键：F12
  - 点击右键，选择“检查”
- （2）2个常用模块的使用
  - Elements 元素定位与分析
  - network 网络请求监控与分析

![image-20200829222519569](/image-20200829222519569.png)

#### 3.1.2 代码规范

- （1）编码规范

```python
# coding = 'utf-8'
或
# _*_ coding:utf-8 _*_
```

- （2）自身调用

```python
if __name__ == "__main__":
    # 程序身上调用
    pass
```

- （3）代码注释

​        python 使用`#`号类进行代码的单行注释说明，批量注释常用快捷键`ctrl+/`

#### 3.1.3 引入模块

- （1）什么是模块

​       Module 主要是用来从逻辑上来组织python代码（变量、函数、类）的，其本质上就是一个py文件，以便用来提高代码的可扩展性和可维护性。

pyton 使用 `import` 来导入模块，如

```python
import sys
import os
import urllib.request
import re
from bs4 import BeautifulSoup
```

- （2）什么是包（package）

​        package 包的存在是为了避免 **<u>“模块名”</u>** 的冲突，从而python引入了目录结构来组织模块的方法。也就是说包的实质概念就是 **“<u>存放模块的文件夹</u>”**。

- （3）模块类型
  - 原生模块（sys,os,re,urllib）
  - 第三方模块（bs4）
  - 自定义模块（含package概念）

- （4）各种模块引入方式

```python
# 1）原生模块导入（直接导入即可）
import re
import sys,os

# 2）第三方模块导入（首先需要下载安装，然后才能导入：pip install bs4）
import bs4 # 导入bs4第三方包
from bs4 import BeautifulSoup #导入第三方bs4包中的 BeautifulSoup模块

# 3）自定义模块导入（新建一个文件夹作为包:package_test，然后在此文件夹中创建新的test1.py文件作为模块）
from package_test import test1	# 

```



---

### 3.2 获取数据

​         通过HTTP库向目标站点发起请求，请求可以包含额外的headers等信息，如果服务器能正常响应，会得到一个Response，便是所要获取的页面内容。

#### 3.2.1 urllib 网页下载实例

```python
import urllib.request

# 1. 准备请求数据
url = "https://movie.douban.com/top250"
headers = {"user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36"}

# 2. 封装请求对象
request = urllib.request.Request(url, headers=headers)

# 3. 发起请求，获取响应对象
response = urllib.request.urlopen(request)

# 4. 读取源码，并解码
html_page = response.read().decode()

print(html_page)
```



### 3.3 解析内容

得到的内容可能是HTML、json等格式，可以用页面解析库、正则表达式等进行解析提取。

#### 3.3.1 BeutifulSoup解析

BeautifulSoup 是一个第三方的文档树对象解析模块，其可以帮助我们将html或xml文件对象解析为一个python环境下可操作的文档树对象，从而方便我们进行文档内容 <u>**“遍历”**</u> 和 <u>**“查找”**</u>。

##### 3.3.1.1 标签遍历

**<u>标签遍历方法，其主要作用就是对第一个匹配标签和标签中的特定内容进行提取。</u>**当然其实可以对第一个匹配特定标签的上下、左右、兄弟、子孙标签节点内容进行提取（但是，实际使用很少）。

```python
soup.tag

soup.tag.string
soup.a.string

soup.tag.contents
soup.head.contents

soup.tag.name
soup.tag.attrs
```

**<u>使用缺点：</u>** 只能提取到第1个标签，若果要求提取所有某个标签，或者匹配第2个，第3个，操作起来就非常的麻烦，需要一层一层的去遍历。（后面的“搜索查找”的方式就弥补了遍历的不足，2者可以结合的使用。）

##### 3.3.1.2 标签搜索

###### （1）标签查找法: soup.find_all()

标签查找法是BeautifuSoup模块最常用的标签提取方法，本方法即通过  **“<u>soup.find_all(name, attrs, recusive, text, kwargs)</u>”**指定查找的条件进行整个文档树的搜索查找，提取出符合条件的所有标签，**<u>输出一个列表对象</u>**。

> - 1. **name 查找**
>
>   - （i）soup.find_all(‘a')
>   - （ii）soup.find_all(re.compile('a'))
>   - （iii）soup.find_all(['a', 'b'])
>
> - 2. **attrs 查找**
>   
>   - soup.find_all(id="link")
>   - soup.find_all(id=True)
> - 3. **text 查找**
>   
>   - （i）soup.find_all(text="Elsie")
>   - （ii）soup.find_all(text=["Elsie","Lacie","Tillie"])
>   - （iii）soup.find_all(text=re.compile(".+e"))
> - 4. **limit 限制条件补充**
>   
>   - soup.find_all("a", limit=2)



###### （2）选择器查找法: soup.select()

选择器查找法，即通过 **<u>"soup.select()"</u>** 方法指定相应的匹配查找条件对需要查询提取的 **<u>“标签”</u>** 或 **<u>“内容”</u>** 进行定位提取。

> - 1. 通过标签名查找
>
>   - soup.select('a')
>
> - 通过类名查找
>
>   - soup.select(".story")
>
> - 通过id查找
>
>   - soup.select(#link1)
>
> - 通过属性查找
>
>   - soup.select("a[cleass='sister']")
>
> - **通过标签路径查找（最常用）**
>
>   - soup.select("head > title")
>
> - 通过兄弟标签查找字字符串内容
>
>   - tag_lst = soup.select(".title~ .story")
>   - tag_lst[0].get_text()



###### （3）字符串提取补充

- **get_text() **

> 其用于提取标签中 **”字符串内容“** 。我们一般快速的搜索某tag不是最终目标，提取标签内部 ”字符串内容“可能是非常重要的目标之一。

- 常用场景
  - tag.text()
  - soup.a.get_text()
  - soup.find('a',  class_='item').get_text()



#### 3.3.2 正则匹配

###### （1）什么是正则表达式

- **什么是 Regular Expression ?**

**正则表达式（Regular Expression）**：其实就是一个 **<u>“字符串模板”</u>**（这个 “字符串模板”的实质就是表示 ”某一类字符串的集合“，简称 pattern）。其通常用来快速去匹配检查 **<u>“某一个字符串”</u>** 中的 **<u>“子字符串”</u>** 内容，以方便我们进行提取或者替换操作。

- **实际使用场景**

其实际使用场景就是创建一个 **<u>"字符串模板"</u>**， 然后用这个“**<u>字符串模板</u>**”去匹配 **<u>“某个字符串”</u>**中的某个 **<u>”子字符串“</u>**，以便快速检查和提取出所需要字符串内容；



###### （2）常用操作符

| 操作符 | 描述                                                 | 举例                                                         |
| ------ | ---------------------------------------------------- | ------------------------------------------------------------ |
| .      | 匹配任何一个字符串（除换行符外）                     | pattern = re.findall(r".+", str_obj, re.S)：表示匹配任何字符，同时使用忽略换行符的匹配模式。 |
| []     | 一个字符的集合表示                                   | [a-z] ， [a,b,d]                                             |
| [^]    | 一个字符的集合的“非”表示，即反向集合。               | [^ a,b] : 表示非字符 a, b意外的任何字符；                    |
| \d     | 匹配任何一个数字，等价[0-9]                          |                                                              |
| \w     | 匹配任何一个字母与数字字符, 等价[a-z A-Z 0-9]        |                                                              |
| +      | 前一个字符串对象，匹配0次以上；                      | .+ ，.+?                                                     |
| *      | 前一个字符串对象，匹配0次，或0次以上；               | .* ， .*?                                                    |
| ?      | 前一个字符串对象，仅匹配1次或者0次。                 |                                                              |
| {m}    | 前一个字符串对象，匹配 m 次                          | a{3}                                                         |
| {m,}   | 前一个字符串对象，匹配m 到 n次                       | a{3,5}                                                       |
| \|     | 左右表达式任意一个                                   | abc\|efg                                                     |
| ()     | 分组标记，仅提取括号以内字符串；内部常用 \| 结合使用 | （abc）表示abc，(abc\|efg) 表示abc、efg                      |
| ^      | 匹配字符串开头                                       | ^abc                                                         |
| $      | 匹配字符串结尾                                       | efg$                                                         |



###### （3）常用匹配模式

正则表达式可以包含一些可选标志修饰符来控制匹配的模式。

修饰符被指定为一个可选的标志。

多个标志可以通过按位OR(|）它们来指定，如re.l | re.M被设置成Ⅰ和M标志:

| 修饰符   | 描述                                                         |
| -------- | ------------------------------------------------------------ |
| **re.I** | **<u>匹配时，忽略大小写敏感；</u>**                          |
| **re.S** | **<u>匹配包括“换行符”在内容的所有字符；</u>**                |
| re.U     | 根据Unicode字符集分析字符。这个标志影响\w, \W, \b, \B        |
| re.X     | 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解。 |
| re.M     | 多行匹配，影响^和$                                           |
| re.L     | 做本地化识别(locale-aware)匹配                               |



```python
# 匹配查找字符串时，忽略大小写；
re.findall(r'.+', re.I)

# 匹配查找字符串时，将“换行符”包含在内；
re.findall(r'.+, re.S)
```



###### （4）常用方法

-  **search()**
  - re.search(pattern, string_obj)
  - pattern.search("string_obj")

-  **findall()** 
  - re.findall(pattern, string_obj)
  - pattern.findall("string_obj")



###### （5） 实例演示

- 电话号码匹配：`Phone：123-1234-1234`

```python
import re

tel = 'Phone：123-1234-1234'

# 电话号码匹配模式（字符串模板生成）
tel_pattern = re.compile(r'\d{3}-\d{4}-\d{4}')

# （1）使用 search 方法，获取一个match对象
tel_match = tel_pattern.search(tel)

# （2）findall 方法，获取一个列表对象
tel_list = tel_pattern.findall(tel)

print(tel_match)
print(tel_match.group(0))
print(tel_list)

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
打印输出结果：
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<re.Match object; span=(6, 19), match='123-1234-1234'>
123-1234-1234
['123-1234-1234']
```



- 邮政编码匹配：`Post Number：230000`

```python
import re

post = 'Post Number：230000'

# 邮政编码匹配模式生成（字符串模板生成）
post_pattern = re.compile(r'\d{6}')

# （1）search 方法提取
post_match = post_pattern.search(post)

# （2）findall 方法提取
post_list = post_pattern.findall(post)

print(post_match)
print(post_match.group(0))
print(post_list)

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
打印输出结果：
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<re.Match object; span=(12, 18), match='230000'>
230000
['230000']
```



- 电子邮箱匹配：`E-Mail：test@163.com`

```python
import re

E_Mail = 'E-Mail：test@163.com'

# 电话号码匹配模式（字符串模板生成）
mail_pattern = re.compile(r'：([.+]@163.com)')

# （1）search 方法提取
mail_match = mail_pattern.search(E_Mail)

# （2）findall 方法提取
mail_list = mail_pattern.findall(E_Mail)

print(mail_match)
print(mail_match.group(1))
print(mail_list)

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
打印输出结果：
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<re.Match object; span=(6, 19), match='：test@163.com'>
test@163.com
['test@163.com']
```



### 3.4 保存数据

​        保存形式多样，可以存为文本，也可以保存到数据库，或者保存特定格式的文件。

#### 3.4.1 excel 文件保存

```python
# 导入模块
import xlwt

# Create workbook obj
workbook = xlwt.Workbook(encoding='utf-8')

# Create worksheet
worksheet = workbook.add_sheet('top250')

# Write
i = 0
for data in data_lst:
    for j in len(data):
        worksheet.write(i, j , data[j])
    i += 1

# Save
workbook.save('./top250.xls')
```



#### 3.4.2 sqlite 数据库保存

```python
# 导入模块
import sqlite3

# 1. Create a db connection
conn = sqlite3.connection('top250.db')

# 2. Create a cursor
cursor = conn.cursor()

# 3. execute sql
# (1) create table 
create_table_sql = """
		CREATE TABLE MOVIE (
        ID INT PRIMARY KEY NOT NULL,
        movie_link CHAR(100),
        img_link CHAR(100),
        chinese_name CHAR(50),
        foreign_name CHAR(50),
        general_info CHAR(200),
        count INT,
        desc CHAR(100));
"""

# (2) Insert data
for data in data_lst:
    row = """{0},'{1}','{2}','{3}',"{4}","{5}",{6},"{7}" """.format(data_lst.index(data), data[0], data[1], data[2], data[3], data[4], data[5], data[6])
    print(row)

  	insert_data_sql = """
  	INSERT INTO MOVIE (ID, movie_link, img_link, chinese_name, foreign_name, general_info, count, desc) VALUES ({})""".format(row)

# (3) execute sql
cursor.execute(create_table_sql)
cursor.execute(insert_data_sql)

# 4. commit
conn.commit()

# 5. close
conn.close()

```





### 3.5 数据可视化



